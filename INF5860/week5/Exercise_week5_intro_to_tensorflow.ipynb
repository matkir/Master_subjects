{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Introduction to TensorFlow\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This weeks exercise will walk you through the basics of Tensorflow. The goal is for you to be familiar with:\n",
    "- How to work with a computational graph\n",
    "- The tensorflow graph\n",
    "- How to run a graph using tf.Session\n",
    "- Defining various nodes such as variables, constant and placeholders\n",
    "- Defining various operations as add, matmul, etc\n",
    "- The structure of how to implement a nerual network in tensorflow\n",
    "\n",
    "Links:\n",
    "- [Task1: Computational graph](#Task1)\n",
    "- [Task2: Math operations in tensorflow](#Task2)\n",
    "- [Task3: Play with cifar10 in tensorflow](#Task3)\n",
    "\n",
    "\n",
    "\n",
    "Software verion:\n",
    "- Python 3.6\n",
    "- Tensorflow 1.4.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Task1'></a>\n",
    "### Task1: Computational graph\n",
    "---\n",
    "\n",
    "Here you should compute the forward and the backward pass of the given computational graph. \n",
    "\n",
    "The graph represent the sigmoid function:     $\\sigma(\\vec{x}, \\vec{w}) = \\frac{1}{1 + exp(-[w_0x_0+w_1x_1+w_2])}$\n",
    "\n",
    "The green values are the input values which should be propagated forward, and the red values are the values you should propagate backward (the gradients). You should compute the values for the forward and backward pass for all nodes as we did in the lecture. \n",
    "\n",
    "![title](images/sigmoid_graph.png)\n",
    "\n",
    "\n",
    "---\n",
    "Simple example from the lecture:\n",
    "\n",
    "<img src=\"images/simple_graph_example.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To do\n",
    "# Draw the computational graph of the sigmoid function and fill in the values for the forward and backward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Task2'></a>\n",
    "### Task2: Math operations in tensorflow\n",
    "---\n",
    "In this exersice you shall implement mathematical operations in tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mathias/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def math1(mat):\n",
    "    \"\"\"\n",
    "    Square each value in mat separately\n",
    "    \"\"\"\n",
    "    res=np.square(mat)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.   1.   4.   9.]\n",
      " [ 16.  25.  36.  49.]\n",
      " [ 64.  81. 100. 121.]\n",
      " [144. 169. 196. 225.]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "mat = tf.convert_to_tensor(value=np.arange(16).reshape((4, 4)),dtype=tf.float32)\n",
    "res = math1(mat)\n",
    "with tf.Session() as sess:\n",
    "    #print(sess.run(mat))    \n",
    "    print(sess.run(res))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output\n",
    "```python\n",
    "[[   0.    1.    4.    9.]\n",
    " [  16.   25.   36.   49.]\n",
    " [  64.   81.  100.  121.]\n",
    " [ 144.  169.  196.  225.]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def math2(mat):\n",
    "    \"\"\"\n",
    "    Return the sum of all the values in mat\n",
    "    \"\"\"\n",
    "    res=tf.reduce_sum(mat)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120.0\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "mat = tf.convert_to_tensor(value=np.arange(16).reshape((4, 4)),dtype=tf.float32)\n",
    "res = math2(mat)\n",
    "with tf.Session() as sess:\n",
    "    #print(sess.run(mat))    \n",
    "    print(sess.run(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output\n",
    "\n",
    "120\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def math3(mat):\n",
    "    \"\"\"\n",
    "    Return the sum of each row in mat\n",
    "    \"\"\"\n",
    "    res=tf.reduce_sum(mat,1)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6. 22. 38. 54.]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "mat = tf.convert_to_tensor(value=np.arange(16).reshape((4, 4)),dtype=tf.float32)\n",
    "res = math3(mat)\n",
    "with tf.Session() as sess:\n",
    "    #print(sess.run(mat))\n",
    "    print(sess.run(res))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output\n",
    "\n",
    "[ 6 22 38 54]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def math4(mat):\n",
    "    \"\"\"\n",
    "    Return a scaled version of mat, so that it sums to 1\n",
    "    \"\"\"\n",
    "    res = np.divide(mat,math2(mat))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.00833333 0.01666667 0.025     ]\n",
      " [0.03333334 0.04166667 0.05       0.05833333]\n",
      " [0.06666667 0.075      0.08333334 0.09166667]\n",
      " [0.1        0.10833333 0.11666667 0.125     ]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "mat = tf.convert_to_tensor(value=np.arange(16).reshape((4, 4)),dtype=tf.float32)\n",
    "res = math4(mat)\n",
    "with tf.Session() as sess:\n",
    "    #print(sess.run(mat))    \n",
    "    print(sess.run(res))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output\n",
    "```\n",
    "[[ 0.          0.00833333  0.01666667  0.025     ]\n",
    " [ 0.03333333  0.04166667  0.05        0.05833333]\n",
    " [ 0.06666667  0.075       0.08333333  0.09166667]\n",
    " [ 0.1         0.10833333  0.11666667  0.125     ]]\n",
    " ```\n",
    " ---   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def math5(mat, v):\n",
    "    \"\"\"\n",
    "    Element-wise multiply each column in mat with vector v\n",
    "    \"\"\"\n",
    "    res=np.multiply(mat[:],v)\n",
    "    print(v)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_1:0\", shape=(4,), dtype=float32)\n",
      "[[ 0.  1.  4.  9.]\n",
      " [ 0.  5. 12. 21.]\n",
      " [ 0.  9. 20. 33.]\n",
      " [ 0. 13. 28. 45.]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "mat = tf.convert_to_tensor(value=np.arange(16).reshape((4, 4)),dtype=tf.float32)\n",
    "vec = tf.convert_to_tensor(value=np.arange(4),dtype=tf.float32)\n",
    "res = math5(mat,vec)\n",
    "with tf.Session() as sess:\n",
    "    #print(sess.run(mat))    \n",
    "    #print(sess.run(vec))\n",
    "    print(sess.run(res))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output\n",
    "```\n",
    "[[ 0  0  0  0]\n",
    " [ 4  5  6  7]\n",
    " [16 18 20 22]\n",
    " [36 39 42 45]]\n",
    " ```\n",
    " --- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def math6(mat, v):\n",
    "    \"\"\"\n",
    "    Element-wise multiply each row in mat with vector v\n",
    "    \"\"\"\n",
    "    res=None\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "mat = tf.convert_to_tensor(value=np.arange(16).reshape((4, 4)),dtype=tf.float32)\n",
    "vec = tf.convert_to_tensor(value=np.arange(4),dtype=tf.float32)\n",
    "res = math6(mat,vec)\n",
    "with tf.Session() as sess:\n",
    "    #print(sess.run(mat))    \n",
    "    #print(sess.run(vec))\n",
    "    print(sess.run(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output\n",
    "```\n",
    "[[ 0  1  4  9]\n",
    " [ 0  5 12 21]\n",
    " [ 0  9 20 33]\n",
    " [ 0 13 28 45]]\n",
    " ```\n",
    " ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def math7(mat, v):\n",
    "    \"\"\"\n",
    "    Matrix multiply matrix mat with vector v\n",
    "    \"\"\"\n",
    "    #Hint - check dimention on v\n",
    "    res=None\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "mat = tf.convert_to_tensor(value=np.arange(16).reshape((4, 4)),dtype=tf.float32)\n",
    "vec = tf.convert_to_tensor(value=np.arange(4),dtype=tf.float32)\n",
    "res = math7(mat,vec)\n",
    "with tf.Session() as sess:\n",
    "    #print(sess.run(mat))    \n",
    "    #print(sess.run(vec))\n",
    "    print(sess.run(res))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output\n",
    "\n",
    "[14 38 62 86]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='Task3'></a>\n",
    "### Task3: Play with cifar10 in tensorflow\n",
    "\n",
    "---\n",
    "\n",
    "This exercise does not require coding. We want you to study the code, and try to understand how a neural network is built up from the ground in tensorflow. Before you can start, you need to download the cifar10 dataset. This can be done by running <b>\"get_datasets.sh\"</b> within the <b>\"dataset\"</b> folder, or by downloading the dataset (CIFAR-10 python version) from: https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "\n",
    "The cifar10 dataset have 10 classes: [airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck].  \n",
    "The training set consists of 5000 images and the test set consists of 500 images. The images are of size [32,32,3].\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>\"%matplotlib inline\"</b> is used to plot figures within the jupyter notebook\n",
    "- <b>\"tf.reset_default_graph()\"</b> is added to clear the TensorFlow graph from any previous nodes/operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from utils import dataClass\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "%matplotlib inline\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The following cell creates an instance of the class dataClass. The instance <b>\"myData\"</b> loads the cifar10 images. Every image is flattened to an array of size $[32\\cdot32\\cdot3+1, 1] = [3073, 1]$. The \"+1\" is a row of ones to accommodate for the bias. The dataClass have useful functions:\n",
    "- next_training_batch(batch_size)\n",
    "- get_test_data()\n",
    "\n",
    "\n",
    "To be able to feed the training and the test data into the tensorflow graph, we define <b>\"tf.placeholders\"</b> for the data and the corresponding labels (onehot format). The <b>\"global_step\"</b> variable will be used to count the number of training iterations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load cifar10 data\n",
    "cifar10_dir = 'datasets/cifar-10-batches-py'\n",
    "myData      = dataClass.dataClass(cifar10_dir)\n",
    "\n",
    "#Define placeholders for being able to feed data to the tensorflow graph\n",
    "data          = tf.placeholder(shape=(None, myData.numbOfFeatures), dtype=tf.float32, name='data')\n",
    "labels_onehot = tf.placeholder(shape=(None, myData.numbOfClasses),  dtype=tf.int32,   name='labels_onehot')\n",
    "global_step   = tf.Variable(initial_value=0, trainable=False, name='global_step')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Lets define the structure of the tensorflow graph. The number and size of the hidden layers can be modified by changing the <b>\"hiddenLayerSizes\"</b> list. Default, the neural network is a fully connected neural network (dense neural network) with two hidden layers of size 1024 and 265. We encurrage you to play with the network configuration. For example, a fully connected neural network with 3 hidden layers can be defined by editing <b>\"hiddenLayerSizes\"</b> to:\n",
    "\n",
    "hiddenLayerSizes = [myData.numbOfFeatures, 1024, 265, 128, myData.numbOfClasses]\n",
    "\n",
    "You can also try to change the initialization of the weights and the activation function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# N -> number of training samples\n",
    "# D1 -> number of input features\n",
    "# D2 -> number of output features\n",
    "# C -> number of output features\n",
    "\n",
    "# - W: A array of shape (D1, D2) containing weights.\n",
    "# - data: A array of shape (N, D1) containing a minibatch of data.\n",
    "# - labels_onehot: A array of shape (N, C) containing training labels\n",
    "\n",
    "# Lets define a fully connected neural network\n",
    "hiddenLayerSizes = [myData.numbOfFeatures, 1024, 265, myData.numbOfClasses]\n",
    "a = data\n",
    "for ii in range(len(hiddenLayerSizes)-1):\n",
    "    layerName = 'layer%s' % ii\n",
    "    with tf.variable_scope(layerName):\n",
    "        ny = hiddenLayerSizes[ii]\n",
    "        nx = hiddenLayerSizes[ii+1]\n",
    "        W  = tf.get_variable(name='W', shape=(ny, nx), initializer=tf.contrib.layers.xavier_initializer())\n",
    "        z  = tf.matmul(a, W, name='matmul')\n",
    "        a  = tf.tanh(z, name='activation_function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The loss is computed using the built in tensorflow function <b>\"tf.losses.softmax_cross_entropy\"</b>. It calculates the softmax cross entrpy loss. If you want to improve the generalization of the network, you could try to add regularization loss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define your loss function\n",
    "logits  = a\n",
    "loss    = tf.losses.softmax_cross_entropy(onehot_labels=labels_onehot, logits=logits)\n",
    "regloss = 0\n",
    "losses  = loss + regloss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We define a gradient descent optimizer. We pass in the loss (losses) we want to minimize, and the list of the variables (weights) we want to minimize the loss with respect to. The minimizer returns an operation which we call, \"train_op\". Every time we want to perform a gradient descent step we will call <b>\"train_op\"</b> in the tf.Session.\n",
    "\n",
    "The <b>\"global_step\"</b> variable is passed into the minimizer and is incremented for every gradient descent step.\n",
    "\n",
    "We would like you to play with the learning_rate. Default <b>\"learning_rate=0.05\"</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define an optimizer\n",
    "all_variables = tf.trainable_variables()\n",
    "optimizer     = tf.train.GradientDescentOptimizer(learning_rate=0.05)\n",
    "train_op      = optimizer.minimize(losses, global_step=global_step, var_list=all_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The accuracy measure <b>\"accuracy\"</b> is calculated. Other possible measures could be: recall, precision, f1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calculate the accuracy\n",
    "estimated_class = tf.argmax(logits, axis=1)\n",
    "labels          = tf.argmax(labels_onehot, axis=1)\n",
    "accuracy        = tf.reduce_mean(tf.cast(tf.equal(estimated_class, labels), tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Here is where the action takes place! The cell creates a <b>\"tf.Session\"</b> and trains the nerual network by calling the <b>\"train_op\"</b>. Note, see how we use the two placeholders <b>\"data\"</b> and <b>\"labels_onehot\"</b> to feed the graph with new training images/labels. If training takes a long time try to reduce <b>\"numbOfTrainingSteps\"</b>.\n",
    "\n",
    "You can try to play with the hyperparameters:\n",
    "- numbOfTrainingSteps\n",
    "- batch_size \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "numbOfTrainingSteps = 10000\n",
    "batch_size          = 500\n",
    "\n",
    "#Log train loss/accuracy and test loss/accuracy\n",
    "train_loss     = np.zeros(numbOfTrainingSteps)\n",
    "train_accuracy = np.zeros(numbOfTrainingSteps)\n",
    "test_loss     = []\n",
    "test_accuracy = []\n",
    "test_inds     = []\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer()) \n",
    "    timeZero = time.time()\n",
    "    for ii in range(numbOfTrainingSteps):\n",
    "        npData, npLabels_onehot   = myData.next_training_batch(batch_size)\n",
    "        loss_val, accuracy_val, _ = sess.run([loss, accuracy, train_op],\n",
    "                                             feed_dict={data: npData, labels_onehot: npLabels_onehot})\n",
    "        train_loss[ii]         = loss_val\n",
    "        train_accuracy[ii]     = accuracy_val\n",
    "\n",
    "        #Block is printing accuracy, loss and ETA.\n",
    "        if ii % 50 == 0:\n",
    "            currentTime = time.time()-timeZero\n",
    "            secPerIter  = currentTime/(ii+1)\n",
    "            remTime     = (numbOfTrainingSteps - ii)*secPerIter\n",
    "            remMin      = int(remTime/60)\n",
    "            remSec      = remTime%60\n",
    "            print('Accuracy=%f | loss=%f | ETA: min=%d, sec=%d' % (train_accuracy[ii], train_loss[ii], \n",
    "                                                               remMin, remSec))\n",
    "        \n",
    "        #Block is calculating test accuracy and loss\n",
    "        if ii % 100 == 0:\n",
    "            npData, npLabels_onehot = myData.get_test_data()\n",
    "            loss_val, accuracy_val, _ = sess.run([loss, accuracy, train_op], \n",
    "                                                 feed_dict={data: npData, labels_onehot: npLabels_onehot})\n",
    "            test_loss.append(loss_val)\n",
    "            test_accuracy.append(accuracy_val)\n",
    "            test_inds.append(ii)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We plot the loss and the accuracy as a function of gradient descent steps to monitor the training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training accuracy and the training loss\n",
    "#plt.figure()\n",
    "plt.figure(figsize=(18, 16), dpi= 80, facecolor='w', edgecolor='k')\n",
    "ax = plt.subplot(2, 1, 1)\n",
    "# plt.subplots_adjust(hspace=2)\n",
    "ax.plot(train_loss, 'b', label='train_loss')\n",
    "ax.plot(np.array(test_inds), np.array(test_loss), 'r', label='test_loss')\n",
    "ax.grid()\n",
    "plt.title('Loss', fontsize=20)\n",
    "plt.ylabel('Loss', fontsize=18)\n",
    "plt.xlabel('Iterations', fontsize=18)\n",
    "ax.legend(loc='upper right', fontsize=15)\n",
    "\n",
    "ax = plt.subplot(2, 1, 2)\n",
    "plt.subplots_adjust(hspace=0.7)\n",
    "ax.plot(train_accuracy*100, 'b', label='train_accuracy')\n",
    "ax.plot(np.array(test_inds), np.array(test_accuracy)*100, 'r', label='test_accuracy')\n",
    "ax.grid()\n",
    "plt.title('Accuracy', fontsize=20)\n",
    "plt.ylabel('Accuracy [%]', fontsize=18)\n",
    "plt.xlabel('Iterations', fontsize=18)\n",
    "ax.legend(loc='lower right', fontsize=15)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
